\section{Synthetic Experiments}

In this section we describe our results on extensive synthetic experiments performed with our model and benchmark methods in two conditions:
1) views missing at random for each dataset,
and 2) datasets with systematically missing views (missing not at random).

%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% SYNTHETIC EXPERIMENTS %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Data preparation}
\label{ssec:synth}

To simulate multi dataset observations, we sample the latent variable $\z_{d,n}$ from a multivariate Gaussian with zero-mean and identity covariance matrix, and then we transform it with random linear mapping towards the observation space to get $\xdnv$.
We then corrupt the observation with increasing levels of noise
and we apply specific strategies to remove views in the context of the \textit{missing at random} (MAR) and \textit{missing not at random} (MNAR) experiments.

%% MAR %%
In the MAR experiments views were randomly removed according to a parameter $0 \leq f \leq 1$ to control the fraction of data-points with complete views.
In the extreme case $f=1$, all the data-points have all the views, representing the ideal case of no missing views, that is the working case of the Multi-Channel Variational Autoencoder \citep{Antelmi2019}.
In the case $f=0$, each data-point has only one randomly assigned view, representing the case where no relationship between views can be established, where the our multi-view model collapses to a disjoint series of independent Variational Autoencoders \citep{Kingma2013, Rezende2014}.
In the general case, each data-point has probability $f$ to have all the views, and probability $1-f$ to have a randomly assigned view out of the total available views.
The general case represents the case where the relationship between views can be established only through a fraction $f$ of the total available data-points.

%% MNAR %%
In the MNAR experiments we removed specific views for each simulated dataset, ensuring at the same time the absence of at least one view for a datasets, and the presence of at least one view in common between pairs of datasets.
As an example, in the case with three datasets and three views, the association view-dataset can be expressed through the following association matrix $A$:
\begin{equation}
A = 
\begin{pmatrix}
1 & 0 & 1 \\
1 & 1 & 0 \\
0 & 1 & 1 
\end{pmatrix},
\end{equation}
where $A(v,d)=1$ indicates the presence of view $v$ in dataset $d$.
We limited our MNAR simulations to cases with square association matrices with maximum dimensionality of $5$.

\subsection{Model Fitting and Evaluation}

In both MAR and MNAR experiments we fit the synthetic scenarios with our model, where we choose a linear Gaussian parametrization for the variational and likelihood distributions, such that respectively:
\begin{align}
\label{eq:encoder}
q_{d,n,w}(\z) &= \Gaussof{\mub=\mathbf{V}_w^{(\mu)} \xdnw, \Sigmab = diag(\mathbf{V}_w^{(\sigma)} \xdnw)}, \\
\label{eq:decoder}
\p{\xdnv|\z,\thetab_v} &= \Gaussof{\mub = \mathbf{G}_v^{(\mu)} \zb, \Sigmab = diag(\mathbf{g}_v^{(\sigma)})},
\end{align}
\textit{i.e.} factorized multivariate Gaussian distributions whose moments are linear transformations depending on the conditioning variables. \\
$\thetab_v = \{\mathbf{G}_v^{(\mu)}, \mathbf{g}_v^{(\sigma)}\}$ and $\phib_w=\{\mathbf{V}_w^{(\mu)}, \mathbf{V}_w^{(\sigma)}\}$ are the parameters to be optimized through (\ref{eq:argmax}).
Lastly we predicted for each simulated scenario the missing views according to \eqnref{eq:reconstruction} on testing hold-out datasets.

Results, cross validated $5$ times, were summarized with the \textit{mean squared error} (MSE) metric on testing hold-out datasets for every simulated scenario.
We applied the same evaluation procedure for the following benchmark methods.

\subsection{Benchmark Methods}
Among state of the art multivariate linear and non linear imputation methods, we selected the following competitors as a benchmark:
1) k-Nearest Neighbors (knn) with $k=\set{1, 5}$;
2) Denoising Autoencoder (DAE);
3) Multivariate Imputation by Chained Equations (MICE).

For the knn approach we used the \textit{KNNImputer} method as implemented in the \textit{Scikit-Learn} library \citep{sklearn}.
Here each sample's missing values are imputed using the mean value from $k$ nearest neighbors found in the training set.
Two samples are close if the features that neither is missing are close in terms of Euclidean distance.

The Denoising Autoencoder, as developed by \cite{dae}, is based on an overcomplete deep autoencoder.
It maps input data to a higher dimensional subspace, which in combination with an initial dropout layer to induce corruption, makes the model robust to missing data.
We used the same architecture proposed by the authors, that is three hidden layers for encoder and decoder networks, Tanh activation functions, hyperparameter $\Theta=7$, and dropout $p=0.5$, as they proved to provide consistent better results.

In MICE, as implemented in \cite{mice}, missing values are modeled as a multivariate linear combination of the available features.
This methodology is attractive if the multivariate distribution is a reasonable description of the data, which in our case it is by construction.
MICE specifies the multivariate imputation model on a variable-by-variable basis by a set of conditional densities, one for each incomplete variable.
Starting from an initial imputation, MICE draws imputations by iterating over the conditional densities.

\subsection{Results}
\input{./tex/fig/fig_synthetic_benchmark}

In the synthetic tests our model comes out as the best performer, with a mean MSE improvement compared to best competing method of $17\%$ in MAR cases and $71\%$ in MNAR cases (\figref{fig:synthetic_benchmark_box}).

We notice that DAE is not always better than knn ($k=5$), especially in low \snr\ cases.

Since MICE is computationally expensive, we were able to fit it only on MNAR cases with high \snr\, where it performed poorly (boxplot not shown), while in MAR cases this model did not converge.

In \figref{fig:synthetic_benchmark_pred_box} we show a stratification of MAR experiments results by the fraction $f$ of data-points with complete views.
Here we notice how with already $f \geq 0.25$ we can significantly reduce the prediction error on testing data-points compared to the case $f=0$, where no relationship between views can be established.
Moreover, reaching the ideal case of $f=1$, that is when there are no missing views in the dataset, does not improve significantly the prediction performance of our model.
This finding suggests that collecting a minimum amount of data-points with all the views is enough for our model to capture the joint relationship among views.

%%%%%%%%%%%%%%%%%%%%%
%% MEDICAL IMAGING %%
%%%%%%%%%%%%%%%%%%%%%
\section{Experiments on Medical Imaging Datasets}
\label{ssec:real}

\input{./tex/tab/table_datasets_numbers.tex}
% \newpage
% \input{./tex/tab/table_datasets_cross_validation.tex}
% \newpage
% \input{./tex/tab/table_datasets_prediction.tex}
\input{./tex/tab/table_datasets_prediction_combat.tex}
\input{./tex/tab/table_datasets_prediction_dx.tex}
\input{./tex/tab/table_model_comparison.tex}


% \input{./tex/tab/table_model_comparison.tex}

\subsection{Data Preparation}
Data used in the preparation of this article were obtained from the following sources.
1) From the Alzheimer's Disease Neuroimaging Initiative (ADNI)
\footnote{
\href{http://adni.loni.usc.edu}{adni.loni.usc.edu}. The ADNI was launched in 2003 as a public-private partnership, led by Principal Investigator Michael W. Weiner, MD. For up-to-date information, see \href{www.adni-info.org}{www.adni-info.org}.
}.
2) From MIRIAD dataset \cite{Miriad}, a database of volumetric MRI brain-scans of Alzheimer's sufferers and healthy elderly people.
3) From a local cohort collected at the University Hospitals, having healthy subjects and subjects with various cognitive disorders.

We divided the ADNI dataset into two independent ones:
`adni1', composed by subjects recruited in the initial study,
and `adni2' composed by those subjects subsequently recruited in ADNI-GO, ADNI2, and ADNI3.
Since data modalities and acquisition protocols of `adni1' are different from those of `adni2', we consider these two cohorts as indipendent datasets.

From the four datasets (`adni1', `adni2', `miriad', `local') we grouped the observations into the following views.
1) `clin': containing age and the Mini-Mental cognitive score (MMSE).
2) `mri': brain volumes computed with FreeSurfer \footnote{
\href{https://surfer.nmr.mgh.harvard.edu/}{surfer.nmr.mgh.harvard.edu}
}.
3) `fdg': average brain glucose uptake measured through the analysis of FDG-PET scans.
4) `av45': average brain amyloid uptake measured through the analysis of AV45-PET scans.
5) `tau': average brain protein Tau uptake measured through the analysis of TAU-PET scans.
6) `mri local': this is a view only present in the `local' dataset, composed by measurements on brain gray matter density derived from SPM \cite{Ashburner2000}.
All the imaging measurements are averaged on the regions of interests defined in the Desikan-Killiany atlas \cite{Desikan2006}
In \tabref{table:datasets} we show the number of observations stratified by dataset and modality.
Please note that the only view in common across datasets is the clinical one, composed by MMSE and age features only.

\subsection{Fitting procedure}
For the model architecture, we choose the likelihood function as in \eqnref{eq:decoder}.
We then choose to parametrize the encoder as:
\begin{equation}
\label{eq:dropout_posterior}
    q_{d,n,w}(\z) = \Gauss{\mub = \mathbf{V}_w^{(\mu)} \xdnw}{\Sigmab = \text{diag}(\sqrt{\alphab} \odot \mub)^2}.
\end{equation}
The posterior distribution so defined is known as \textit{dropout posterior}.
The dropout parameter $\alphab$ has components $\alpha_i = \nicefrac{p_i}{1-p_i}$ linked to the probability $p_i$ of dropping out the $i$-th latent variable component \cite{Wang2013}.
It has been shown that the association of this dropout posterior with a log-uniform prior distribution $\pz$ leads to sparse and interpretable models \cite{Antelmi2019,Molchanov2017}.

We fit our multi-view model one dataset at a time, and lastly on the whole dataset pool.
We will refer to this last model as `all'.
Observations were divided into five splits, stratified to keep an intra-dataset proportion of views as close as possible as the ones of the original datasets.
After setting a dropout threshold to $0.5$ we measured, through the mean squared error, the prediction error of the dataset-specific models on the testing hold-out observations of the same dataset (within).
To have a measure of the generalization ability for each model, we measured the prediction accuracy also on all the test sets not belonging to the same dataset of the trainig set (cross).
The same was done with the model `all'.
In this case the hold out observations belong to the whole dataset pool and we will refer to its testing set as the `joint' set.

\subsection{Results}
In \tabref{table:crossvalidation_details} we show for each trained multi-view model, the prediction error on the within-dataset test sets, cross-dataset test sets, and joint set, whenever computable.
We notice that the generalization performance of the model `all' is almost always the best in predicting the `clin' and `mri' views.
These are also the views most represented in all the datasets.
We also note that joint model systematically outperforms the dataset-specific models when applyed across dataset.
Moreover it generally provides better predictions even respect to the within-dataset results.
On the other extreme, views belonging only to a specific dataset, such as `fdg' in `adni2' and `mri local' in the `local' dataset are better predicted with dataset-specific models.
In the remainig cases of `av45' and `tau', the joint model seems, on average, to perform better than the single dataset-specific ones.

