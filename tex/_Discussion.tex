\section{Discussion}

In both the experiments on synthetic and real data, our model compared favorably with respect to state of the art benchmark methods.
As a secondary result, we report the surprising performance of knn (k = 5) in synthetic scenarios, especially in low snr cases, and on real data experiment, were it is most of the time superior to the DAE.
This finding is corroborated by \cite{Platias2020} were knn is found to be superior to methods based on autoencoders.

An interesting result is the one presented in \figref{fig:synthetic_benchmark_pred_box}, suggesting that collecting a minimum amount of data-points with complete views ($25\%$ in our setup) is enough for our model to capture the joint relationship among views.
In our synthetic tests, training on more complete scenarios does not seem to improve significantly the testing results.
This condition may be due to overtraining \citep{Bilbao2017}, where collecting more data is not necessary to improve the model performance.
We think that this fact can have a positive impact in studies were the acquisition of complete observations is hampered by economical or ethical reasons.

The experimental results on real medical imaging datasets (\tabref{tab:features}, \tabref{tab:classifier}) shows the clear improvement of our method with respect to the MCVAE, that inspired our work.
The features and diagnosis prediction clearly improves when using our method, that allows to not discard observations with missing views.
Our method generalizes better to new unseen datasets when trained jointly on multiple datasets, with respect to cases where the training happens on a single dataset.

In our work we have explicitly modeled a one-to-one correspondence between encoding and decoding views.
This makes our model part of the family of the auto-encoders, where the model acts as a identity transformation between the input and the output.
Other architectures are nevertheless possible.
In general, there may be an $m$-to-$n$ relationship, with partially overlapping views among $m$ input views and $n$ output views.
Investigating the properties of all the possible architectures is beyond the scope of this work.

As final remark, we want to stress that our model is based on the assumption of independent and identical distributed observations.
This assumption may be limiting in healthcare datasets, such as the ones used in this work.
In our work we mitigated these biases by harmonizing the datasets before applying our model, and we leave the extension and development of a bias-transparent multi-view model to future works.

