\section{Conclusions}
We proposed a new multi-task latent variable generative model able to learn simultaneously from multiple datasets, even in the presence of non-overlapping views among all the datasets.
The available overlap between pairs of datasets allow the information to flow through all the views in the dataset pool.
Since the learned view-specific parameters are shared among datasets, missing views can be automatically imputed for every dataset.
The method proposed in this work is a coherent extension of classical variational generative models, making the training fast and scalable.
Being dataset agnostic, our method allows to integrate all the available data into a joint model, gathering  all the available information from multiple datasets at the same time.
We conducted extensive tests for the joint modeling of synthetically generated data and of multi-modal neuroimaging datasets from independent dementia studies and associated clinical data, showing the competitiveness of our method with respect to the state of the art.
Thanks to its general formulation, the proposed method can find applications beyond the neuroimaging research field.
