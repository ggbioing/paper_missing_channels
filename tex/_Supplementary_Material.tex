\section*{Supplementary Material}
\label{sec:supmat}
\setcounter{section}{0}
\setcounter{figure}{0}
\setcounter{table}{0}
\setcounter{equation}{0}
\setcounter{algorithm}{0}
\renewcommand{\thesection}{S\arabic{section}}  
\renewcommand{\thefigure}{S\arabic{figure}}
\renewcommand{\thetable}{S\arabic{table}}  
\renewcommand{\theequation}{S\arabic{equation}}  
\renewcommand{\thealgorithm}{S\arabic{algorithm}}  

\section{Derivation of the Lower Bound}
\label{sm:lower_bound}

The exact solution to the inference problem induced by \eqnref{eq:model} is given by the posterior $\p{\z|\set{\xdnv,\thetab_v}_{v=1}^{V_{d,n}}}$, that is not generally computable analytically.
Following \cite{Antelmi2019}, we can nevertheless look for its approximation $\qz$ through \textit{Variational Inference} \citep{Blei2017}.
By introducing the latent variational approximation $\qz$, we can derive the lower bound on the marginal log-likelihood for a single data-point as follows:
\begin{equation}\label{smeq:LB}  % supplementary material equation
\begin{aligned}
\ln \p{\xdnv|\thetab_v} &= \ln \int \p{\xdnv|\z, \thetab_v} \p{\z} \; d\z \\
                        &= \ln \int \frac{\qz}{\qz} \p{\xdnv|\z, \thetab_v} \p{\z} \; d\z \\
                        &= \ln \EE{\qz}{\frac{\p{\xdnv|\z, \thetab_v} \p{\z}}{\qz}} \\
                        &\geq \EE{\qz}{\ln \p{\xdnv|\z, \thetab_v}} - \KL{\qz}{\pz}.
\end{aligned}
\end{equation}
To derive the last line of \eqnref{smeq:LB} we leverage on the \textit{Jensen's inequality} and collect the result into a new expectation term and in the Kullback-Leibler divergence term ($\kl$).

We define the distribution function $\qz$ to depend on a specific dataset $d$, data-point $n$, and view $w$, such that:
\begin{equation}\label{smeq:q}
\begin{aligned}
\qz = q_{d,n,w}(\z) = \q{\z|\x_{d,n,w}, \phib_w},
\end{aligned}
\end{equation}
where $\phib_w$ represents the view-specific variational parameters shared among all datasets.
To force a link among views, we impose the inequality \eqnref{smeq:LB} to hold for any $w$ in $1 \ldots V_{d,n}$.
To do so, we average the right hand side of \eqnref{smeq:LB} across the $V_{d,n}$ views and rewrite \eqnref{smeq:LB} as follows:
\begin{equation}\label{smeq:newLB}
\begin{aligned}
% \ln \p{\xdnv|\thetab_v} &\geq \underbrace{\frac{1}{V_{d,n}} \sum_{w=1}^{V_{d,n}}
%   \EE{q_{d,n,w}(\z)}{\ln \p{\xdnv|\z, \thetab_v}}
% - \KL{q_{d,n,w}(\z)}{\pz}}_{\LBn}. \\
\ln \p{\xdnv|\thetab_v} \geq \LBdnv = \frac{1}{V_{d,n}} \sum_{w=1}^{V_{d,n}} \LBdnvw,
\end{aligned}
\end{equation}
where
\begin{equation}\label{smeq:LBdnvw}
\begin{aligned}
\LBdnvw = \EE{q_{d,n,w}(\z)}{\ln \p{\xdnv|\z, \thetab_v}} - \KL{q_{d,n,w}(\z)}{\pz}
\end{aligned}
\end{equation}
is the lower bound associated to the data-point $\xdn$ when its view $v$ is predicted from its view $w$.

\section{Optimization}
\label{sm:optimization}

We use Algorithm \ref{smalg:optim} to solve \eqnref{eq:argmax}.
\input{./tex/algorithm}
The summation in \eqnref{eq:argmax} is done for every dataset $d$ along all the available data-points $n$ and their specific views $v$.
We note that batching data-points with common views can speed up the computation by reducing the number of second level \textit{for} loop iterations in Algorithm \ref{smalg:optim}.

\section{Data Generation}
\label{sm:data_generation}
Data points with $V$ views $\xdn = \left\{\xdnv\right\}_{v=1}^V$ with $\xdnv \in \realnumbers^{f_v}$ where created
from a common latent code $\z_{d,n} \in \realnumbers^l$ with $l$ latent dimensions
according to the following model:
%
\begin{equation}
\label{eq:sinthetic}
\begin{aligned}
	&\z_{d,n} \sim \GaussStdDim{l}, \\
	&\epsilonb_v \sim \GaussStdDim{f_v}, \\
	&\mathbf{G}_v = diag\left( \mathbf{R}_v \mathbf{R}_v^T \right)^{-1/2} \mathbf{R}_v, \\
	&\xdnv = \mathbf{G}_v \z_{d,n} + \snr^{-1/2} \cdot \epsilonb_v,
\end{aligned}
\end{equation}
%
where  for every view $v$, $\mathbf{R}_v \in \realnumbers^{f_v \times l}$ is a random matrix with $l$ orthonormal columns
(\ie\ $\mathbf{R}_v^T \mathbf{R}_v = \mathbf{I}_l$),
$\mathbf{G}_v$ is the linear generative law,
and $\snr$ is the signal-to-noise ratio.
%
With this choice, the diagonal elements of the covariance matrix of $\xdnv$ are inversely proportional to $\snr$, \ie\,
%
$diag\left(\E{\xdnv \xdnv^T}\right) = (1 + \snr^{-1}) \mathbf{I}_{f_v}$.
%
This generative
%
Scenarios where generated by varying one-at-a-time the dataset attributes, as listed in \tabref{table:simul_params}.
%
\begin{table}[htb]
\caption{Dataset attributes, varied one-at-a-time in the prescribed ranges, and used to generate scenarios according to \eqnref{eq:sinthetic}.
}
\centering
\begin{tabular}{ll}
\toprule
Attribute description          & Iteration list\\
\midrule
Total views ($V$)              & 3 4 5 \\
Features per view ($f_v$)      & 5 10 100 \\
Latent space dimension ($l$)   & 2 4 8 \\
Training Samples               & 100 500 1000 \\
Testing Samples                & 1000 \\
Signal-to-noise ratio ($\snr$) & 1 3 10 100\\
Seed (re-initialize $\mathbf{R}_v$)  & 1 2 3 4 5\\
\bottomrule
\end{tabular}
\label{table:simul_params}
\end{table}
%
