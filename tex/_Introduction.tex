\section{Introduction}

Because of the inherent complexity of biomedical data and diseases,
researchers are required to integrate data across different studies to increase the sample size and obtain better models \citep{LeSueur2020}.
%
When developing integrative models, researchers have to face with multiple concurrent challenges, such as the ones related to
datasets interoperability \citep{Tognin2020},
data heterogeneity \citep{Buch2020},
and data missingness \citep{GolrizKhatami2020}.
%
Emblematic is the case of integrative modeling when datasets come from multi-centric studies in cognitive and neurological disorders,
such as in Alzheimer's Disease (AD).
%
Here the datasets interoperability is hampered by the existence of different protocols between studies.
Because of this, methods whose modeling task are specifically designed on one dataset cannot be directly applied to another one.
%%%%%%%%%%%%%%%%%
%% MULTI-MODAL %%
%%%%%%%%%%%%%%%%%
%\textcolor{blue}{
%	\refnum{\#12}
	Furthermore, at the level of each single dataset, researchers face the challenge of modeling heterogeneous data,
	such as multiple imaging modalities, clinical scores and biological measurements.
	Each of these sources of information represents an important and independent ``\textit{view}'' on the disease or phenomena under investigation.
%}
Efforts to model multi-view data are increasing in the recent biomedical literature \citep{Vieira2020,Venugopalan2021},
where the objective ranges from predicting clinical outcomes \citep{Chen2019,AbiNader2020,Tabarestani2020}
to synthesizing new modalities \citep{Wei2019, Wei2020, Zhou2020}.
The key concept of a shared information space between views is widespread in the literature for the joint model of multi-view data.
%
%\textcolor{blue}{
%	\refnum{\#7}
This is the case for well established multivariate linear methods such as
Canonical Correlation Analysis (CCA),
Partial Least Squares (PLS),
Independent Component Analysis (ICA),
which are some of the most popular methods for multivariate analyses on imaging data, as documented in a multitude of works from the state of the art (see \cite{Liu2014} for a general review).
%}
	 %
%\textcolor{blue}{
%\refnum{\#3}
	 While these studies essentially focus on the general problem of multivariate association modeling, multi-view methods specifically tailored to medical imaging tasks, such as image registration and segmentation have been proposed in parallel.
	 % Qin et al. Unsupervised deformable registration for multi-modal images via disentangled representations
%\refnum{\#18}
	 For example, in \cite{Qin2019} the authors propose
	 a registration method for aligning intra-subject multi-view images.
	 Although limited to a two images registration setting,
	 in this work views are projected into a common latent space.
	 The proposed registration approach is then built on the latent code and on an image-to-image translation approach.
	 % It is limited to a two-modality setting and assumes no missing data in the training phase.
	 % This work is not a competitor as our method is not meant to solve the registration problem.
	 %
	 %
	 % Chartsias et al. Disentangle, align and fuse for multimodal and zero-shot image segmentation
%\refnum{\#19}
	 \cite{Chartsias2021} propose a segmentation method based on the learning of information presented jointly in complementary imaging views.
	 From the different inputs views, anatomical factors are encoded into a common latent space and fused to extract more accurate segmentation masks.
	 % It assumes no missing data in training phase.
	 % This work is not a competitor as our method is not meant to solve the segmentation problem.
	 %
	 %
	 % Yang et al. Cross-modality segmentation by self-supervised semantic alignment in disentangled content space
%\refnum{\#21}
	 In \cite{Yang2020} a cross-modality segmentation pipeline is built around a similar concept.
	 % It assumes no missing data in the training phase.
	 % 
	 % Advantages of multi-view modeling are described also in the machine learning literature for computer vision.
	 % %
	 % % Huang et al. Multimodal unsupervised image-to-image translation
	 % For example, in \cite{Huang2018}
	 % a method for image-to-image translation is proposed.
	 % From one input image in one domain, multiple output images in the second domain are produced via the sampling in the shared latent space.
	 % The absence of a one-to-one relationship between images in the two domains makes this approach unfit for medical applications, though,
	 % where it is important to assign a specific input image belonging to a patient, to another image belonging to the same patient.
	 % % Tran et al. Disentangled representation learning gan for pose-invariant face recognition
	 % In \cite{Tran2017},
	 % multiple single-modality correlated input images (poses from the same subject) are encoded into a shared latent space to produce a single output (pose-invariant) image.
	 % % It assumes no missing data in training nor in testing/validation phase.
	 %
	 %%%%%%%%%%%%%%%%%%
	 %% MISSING DATA %%
	 %%%%%%%%%%%%%%%%%%
%}
%\textcolor{blue}{
%\refnum{\#3, \#7}
	 In all the works cited so far, the problem of missing data, specifically of missing views during training of multi-view methods, is generally not addressed nor considered.
%}
Still, this is a very common problem when joint modeling multiple datasets, especially in neuroimaging research.
At the level of the single dataset, views can be missing at random (MAR) for some subjects.
Typically, as fitting multi-view models requires to establish correspondences between views, observations with at least one missing view are generally discarded, yielding to potentially severe loss of available information.
To mitigate this problem, imputation methods can be applied to infer missing views, by modeling the relationship across views from complete observations.
%
The loss of information is exacerbated when considering multiple datasets altogether.
Indeed, according to the cohort study design, there may be views which are specifically absent for a given dataset, hence missing not at random (MNAR).
This potential mismatch across datasets hampers their interoperability,
and prevents the gathering of all the available observations into a single, robust and generalizable joint model accounting for the global data variability.
%
This challenge is typically addressed in machine learning by the field of Multi Task Learning (MTL).
To address this issue, MTL aims at improving the model interoperative capabilities by exploiting the information extracted from multiple datasets.
In MTL each task is usually associated to the modeling of a specific dataset and its views only,
when the main idea consists is sharing across datasets the parameters learned through each modeling task \citep{Caruana1998, Dorado-Moreno2020}.
As an example of MTL, in model-agnostic meta-learning (MAML) \citep{MAML1} the training of a model on a variety of learning tasks enforces the generalization on new datasets after few fine tuning iterations.
%
In the context of data assimilation, MTL is usually achieved with specific output layers for every task, and by including a shared latent representation for all of them \citep{Dorado-Moreno2020}.
This modeling rationale is at the basis of recent MTL based approaches to heterogeneous data assimilation \citep{Wu2018, Shi2019},
especially in medical imaging approaches.
%
% MULTI-STAGE FOR DIAGNOSIS CLASSIFICATION
%\textcolor{blue}{
%\refnum{\#14}
	For example, in \cite{Zhou2019a}, the authors propose a staged deep learning framework for dementia diagnosis classification,
	able to jointly exploit multi-view data (MRI, PET, and genetic data in this case).
	Their approach, where at each stage the model learns feature representations for different combinations of views, solves elegantly the problem of missing data.
	Although inspiring for their use of the maximum number of available data samples at each stage,
	the combinatorial nature of their framework makes it in practice applicable only for datasets with very few available views.
	For example, when considering $3$ views, this approach requires to learn $7$ networks.
	% $3$ for each view in stage $1$;
	% $3$ for each couple of modalities in stage $2$;
	% $1$ for the only triplet available at stage $3$.
	With $4$ views, the number of networks that need to be trained, considering all the possible couples, triplets and quadruplets of views amounts to $4845$;
	while with $5$ views it exceeds $10^{32}$.
	Moreover, this framework is currently designed for classification tasks only, excluding the possibility of modality-to-modality prediction.
%}
% EMBRACE_NET
%\textcolor{blue}{
%\refnum{\#11}
	With the \textit{EmbraceNet} of \cite{embracenet} the problem of missing data is managed by zero-filling the missing input views and by the application of a specific dropout technique where multinomial samples are used to assign partitions of the latent space to specific views.
	As there are latent features that are randomly discarded even when the correspondent view is not missing, this represents still a loss of information.
	Similarly as for the previous work, the proposed framework is currently applicable in classification tasks only.
%}
% DAE
Dropout is at the basis of the \textit{Denoising Autoencoder}, as developed by \cite{dae}.
Here an overcomplete deep autoencoder maps input views to a higher dimensional space.
The initial dropout layer induces random corruption in the input views, making the model robust to missing data.
This framework is currently applicable in feature prediction tasks only.

The common underlying assumption of these approaches consists in the existence of a
proper transformation into a common latent code for the solution of multiple tasks,
whether classification or feature prediction.
%
Based on this general assumption,
the Multi-Channel Variational Autoencoder (MCVAE) \citep{Antelmi2019}
is a recent  analysis method allowing the identification of a common latent representation for different views belonging to a single dataset \figrefp{fig:architecture}.
MCVAE extends currently available approaches to account for non-linear transformations from the data to the latent space,
while it can be adapted to multiple tasks, including data reconstruction and classification.
In spite of the high modeling flexibility, the extension of this method to the analysis of multiple datasets is currently challenging.
Training the MCVAE in a multi-dataset context is indeed possible with some limitations:
1) after having discarded observations with missing views;
2) when at train time all the observations are compatible in terms of available views.

To overcome these limitations, in this work we investigate an extension of MCVAE to simultaneously learn from multiple datasets, even in the presence of non compatible views between datasets, and missing views within datasets.
While our formulation naturally extends the original MCVAE approach,
to the best of our knowledge no systematic investigation of this approach for the modeling of multi-view and multi-dataset neuroimaging data has been proposed so far.
Our extension is built upon the following steps:
1) defining tasks across datasets based on the identification of data subsets presenting compatible views,
2) stacking multiple instances of the MCVAE, where each instance models a specific task,
3) sharing the models parameters of common views between modeling tasks.
%
Thanks to these actions, the framework here proposed allows to learn a joint model for all the subjects without discarding any information \figrefp{fig:model}.
The common views between tasks act as a bridge and enable the information to flow through all the other views,
while, in the training phase, tasks lacking a particular view will simply not contribute to the learning of those view-specific parameters.
All the tasks will nevertheless benefit from the parameters they didn't contribute to learn, for the prediction of their missing views.
The proposed variational formulation for computing approximate posterior distributions of the latent variables allows fast and scalable training.
Being dataset agnostic, our method allows to integrate all the available data into a joint model, gathering  all the available information from multiple datasets at the same time.

The rest of this paper is structured as follow.
In \S~\ref{sec:method} we present the mathematical derivation of the classical MCVAE model that will be used to derive the proposed framework.
In \S~\ref{sec:proof_of_concept} we show an illustrative application for the joint modeling of MRI and PET images when some modalities are missing in the training phase.
In \S~\ref{sec:synth}, experiments on synthetic data show that the prediction error of missing views is competitive with respect to the one obtained with state of the art imputation methods.
In \S~\ref{sec:real}, experiments on real data from independent multi-modal neuroimaging datasets show that our model generalizes better than dataset-specific models, in both the cases of data reconstruction and diagnosis classification.
Lastly we discuss our results and conclude our work with summary remarks.

% \subsection{Related Works}
% 
% In \citep{Lorenzi2019} \ldots
% In \citep{Vaden2020} data sharing approach that borrows principles and methods from multiple imputation to replace observed values with synthetic values, thereby creating a fully synthetic neuroimaging dataset that accurately represents the covariance structure of the observed dataset. 
%
% This challenge is typically addressed in machine learning by the fields of Transfer Learning (TL) and Multi Task Learning (MTL).
% TL is based on the transfer of model parameters across datasets \citep{TL}.
% This paradigm is commonly applied to datasets with compatible modalities (e.g. from image to image) and in its basic form consists in using the parameters trained on the first dataset to initialize the training on a second dataset.
% Unfortunately, this often leads to the problem known as catastrophic forgetting \citep{CatastroficForgetting}, consisting of neural networks that loose the information learned from the first modeling task after training a second one.
%
% As an example, $k$-Nearest Neighbor is a popular approach based on the imputation from $k$ nearest neighbors found in the observations with full views.
%
