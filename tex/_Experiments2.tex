%%%%%%%%%%%%%%%%%%%%%
%% MEDICAL IMAGING %%
%%%%%%%%%%%%%%%%%%%%%
\section{Experiments on Medical Imaging Datasets}
\label{sec:real}

% \input{./tex/tab/table_datasets_cross_validation.tex}
% \input{./tex/tab/table_datasets_prediction.tex}
% \input{./tex/tab/table_model_comparison.tex}

Joint modeling of multiple datasets in real applications often represents an hybrid case between modeling missing at random (MAR) and missing not at random (MNAR) observations.
Indeed, every dataset may lack of views which are instead present in others by default.
At the same time, in each dataset, there may be missing views for some data-points due to unknown reasons.

In this section we describe our results on jointly modeling real medical imaging datasets, independently acquired in the context of research studies on human cognitive decline.

We executed two kinds of experiments:
1) benchmark evaluation of our model against the best competing methods from the previous section;
2) multi-view feature prediction with our model on all the available datasets in multiple conditions.
%  3) diagnosis classification accuracy with our model on all the available datasets.

\subsection{Data Sources}
\label{ssec:datasets}

Data used in the preparation of this section were obtained from the following sources.
\begin{enumerate}
%
\item The Alzheimer's Disease Neuroimaging Initiative (ADNI)
\footnote{
\href{http://adni.loni.usc.edu}{adni.loni.usc.edu}.
The ADNI was launched in 2003 as a public-private partnership, led by Principal Investigator Michael W. Weiner, MD. For up-to-date information, see \href{www.adni-info.org}{www.adni-info.org}.
},
a database of brain imaging and related clinical data of cognitively normal subjects, and on patients presenting various degrees of cognitive decline.
%
\item MIRIAD dataset, a database of brain imaging and related clinical data of cognitively normal subjects and patients affected by Alzheimer's disease \citep{Miriad}.
%
\item OASIS-3 dataset, a database of brain imaging and related clinical data of cognitively normal subjects and subjects at various stages of cognitive decline \citep{oasis3}.
%
\item A local cohort collected at the Geneva University Hospitals, with brain imaging and related clinical data of patients with various cognitive disorders.
\end{enumerate}

The available imaging modalities comes from the following acquisitions:
%
\begin{enumerate}
\item structural Magnetic Resonance Imaging (MRI) to measure anatomical volumes in the brain;
%
\item Positron Emission Tomography (PET) with Fluorodeoxyglucose (FDG) tracer, to measure the glucose uptake, which reflecting the functional status of the brain;
\item PET with the AV45 tracer, to measure the amyloid protein uptake in the brain, affecting the functioning of neurons ;
\item PET with the tracer for the TAU protein, also affecting the functionality of neurons.
\end{enumerate}
%

We divided the ADNI dataset into two complementary datasets:
`adni1', composed by subjects recruited in the initial ADNI1 study (2004-2009),
and `adni2' composed by those subjects subsequently recruited in ADNI-GO, ADNI2, and ADNI3 (2010-ongoing).
Since data modalities and acquisition protocols of `adni1' present differences from those of `adni2', we consider these two cohorts as independent datasets.
Specifically, in `adni1' and `adni2' the MRI imaging was performed respectively on 1.5T and 3T scanners.

To summarize, we grouped our data into five distinct datasets which we named as follows: `adni1', `adni2', `miriad', `oasis3', `geneva'.

\subsection{Imaging Processing}
All the brain scans were processed in order to have measurements on regions defined in the Desikan-Killiany atlas \citep{Desikan2006}.
All brain MRI scans were processed with FreeSurfer \footnote{\href{https://surfer.nmr.mgh.harvard.edu/}{surfer.nmr.mgh.harvard.edu}} \citep{freesurfer} to measure brain cortical and sub-cortical volumes, and volumes occupied by the cerebro-spinal fluid (CSF), for a total of $99$ regions of interest.
Relative standardized uptake values (SUVR) were computed for all the PET scans (FDG, AV45, TAU), processed with SPM \citep{Ashburner2000}.
SUVRs were computed using the cerebellum as reference region, and averaged in the same regions used for the MRI, except those containing the CSF, for a total of $94$ regions of interest.

\subsection{Gathering Observations into Views}
\label{ssec:views}
\input{./tex/tab/table_datasets_numbers.tex}

Observations from the five available datasets (\S~\ref{ssec:datasets}) were grouped into the following views.
\begin{enumerate}
\item clin: grouping age and the Mini-Mental cognitive score (MMSE).
\item MRI: grouping brain volumes computed with FreeSurfer.
\item FDG: average brain glucose uptake measured through the analysis of FDG-PET scans.
\item AV45: average brain amyloid uptake measured through the analysis of AV45-PET scans.
\item TAU: average brain protein Tau uptake measured through the analysis of TAU-PET scans.
\end{enumerate}

For each subject belonging to the `adni1', `adni2', 'miriad' and `geneva' datasets, we choose the first available time-point, or baseline.
In `oasis3', since measurements were mostly acquired in different days, we choose to pair nearby time points across modalities into a single one.
Time interval between views within one subject was minimal (AV45 vs MRI: $\leq 90$ days, MRI vs clin: $\leq 90$ days).

In \tabref{table:datasets} we show the number of observations stratified by dataset and view.
Size of the intersection ($\cap$) and union ($\cup$) of subjects with available views is also provided.
Please note that the only view in common across datasets is the clinical one, composed by MMSE and age features only.

We adjusted all the views feature-wise with \textit{ComBat} \citep{combat}, a batch normalization method originally develop in genomics, which has been adopted in neuroimaging studies to reduce unwanted sources of variation in the data due to the differences in acquisition protocols among datasets \citep{Fortin2017, Fortin2018, Orlhac2020}.
In ComBat, we set the `age' as main regressor, and `adni2' as reference dataset for the training set.
As we used cross-validation to validate our results, the ComBat reference dataset for the testing was the training split.

A final feature-wise standardization step was applied by zero centering the data and by rescaling them to have a unity variance.
Standardization parameters were computed on the training sets and applied to training and testing sets.

\subsection{Experiment 1: Benchmark Validation}
\input{./tex/tab/table_model_comparison.tex}

The purpose of this experiment is to validate on real data the benchmarked results obtained with the synthetic experiments (\S~\ref{sec:synth}).

As benchmark methods, we choose the best performers on the synthetic experiments, namely knn5 and DAE.
We choose for our model a linear Gaussian parameterization for the likelihood and variational distributions of \eqnref{eq:decoder} and \eqnref{eq:dropout_posterior} respectively.

We trained all the models (knn5, dae, ours) with data coming from all the datasets except from `adni2', left out for testing purposes.
We choose the `adni2' dataset as testing dataset since it provides all the views, and the highest number of observations per view (\tabref{table:datasets}).

Prediction performances were evaluated with the Mean Squared Error (MSE) metric, measured on the available views in the testing dataset, reconstructed with \eqnref{eq:reconstruction}.
All results were validated by means of $5$-folds cross-validation.

\paragraph{Results}
In \tabref{tab:model_comparison}, we show the MSE metric on predicting missing views in the testing dataset with our model and with the benckmark ones.
Best results are in boldface, which show a clear advantage of using our model and confirm our findings in the synthetic experiments.

\subsection{Experiment 2: Feature Prediction}
\label{ssec:feats}
\input{./tex/tab/table_datasets_prediction_combat_couples.tex}
The purpose of this experiment is to compare the generalization performance of our model with respect to the MCVAE in feature prediction experiments.
This experiment was run in three different conditions:
%
\begin{enumerate}
\item Single Task (ST): when training and testing data are chosen from the same dataset;
%
\item Single Task + Transfer Learning (ST + TL): when models trained on one dataset are tested on another dataset;
%
\item Multi Task Learning (MTL): when models are trained on all the available datasets except the testing one.
%
\end{enumerate}
%
In ST and ST + TL experiments, both MCVAE and our model are trained on the same views,
but while in MCVAE we have to discard observations with missing views from the training set, with our model we can include then by batching together observations with common views into sub-tasks.
In MTL experiments MCVAE cannot be trained because no observation have all the views simultaneously.
%By removing the cases where views are not simultaneously present in both the training and testing set,
%we are left with $16$ possible scenarios where all the three experimental conditions can be simultaneously tested an thus compared.

We choose for both MCVAE and our model a linear Gaussian parameterization for the likelihood and variational distributionsas in \eqnref{eq:decoder} and \eqnref{eq:dropout_posterior} respectively.
All models were trained on all the available views in the training dataset.%, independently of their presence in the testing dataset.
Prediction performances were evaluated with the Mean Squared Error (MSE) metric, measured on the available views in the testing dataset, reconstructed with \eqnref{eq:reconstruction}.
All results were validated by means of $5$-folds cross-validation.

\paragraph{Results}
In \tabref{tab:features} we show the prediction error in terms of MSE for each test dataset and view, on the three experimental conditions described earlier.
In ST and ST + TL cases our model performs either similarly or statistically better than the MCVAE, especially in cases where the difference between the union and intersection set of observations is higher (cfr. \tabref{table:datasets}).
In MTL cases there are $12$ cases that could be fitted with our model.
In $7$ of them we measure a better performance with respect to the ST and  ST + TL cases.

\subsection{Experiment 3: Diagnosis Prediction}
\input{./tex/tab/table_datasets_prediction_dx_couples.tex}

The purpose of this experiment is to investigate the generalization performance of our model with respect to the MCVAE in experiments of diagnosis prediction, with diagnostic classes among:
Alzheimer's disease (AD),
mild cognitive impairment (MCI),
normal cognition (NC).

For both MCVAE and our model we choose a linear Gaussian parameterization for the variational distributions as in \eqnref{eq:dropout_posterior}.
To adapt the models to this new classification experiment, we adopt as decoding function for the latent variable $\z$, the following Categorical likelihood:
\begin{align}
\label{eq:classifier}
\p{y_{d,n}|\z,\thetab} = \operatorname{Cat}\left(\mathbf{\pi}=\thetab\z\right),
\end{align}
where $y_{d,n}$ is the diagnosis associated to the data-point $n$ in the dataset $d$.
The probability vector $\mathbf{\pi}$ is a two dimensional vector representing the class probability for each of the three binary comparisons across the three diagnostic classes, namely AD \textit{vs} MCI, AD \textit{vs} NC, MCI \textit{vs} NC ,and is parametrized with a linear transformation of the latent $\z$ by the matrix $\thetab$.

Models were trained on all the available views in the training dataset, independently of their presence in the testing dataset.
Classes probabilities were inferred from the all the available views in the testing dataset with the following equation:
\begin{equation}\label{eq:reconstructiony}
\begin{aligned}
\hat{y}_{d,n} = \frac{1}{V_{d, n}} \sum_{w=1}^{V_{d, n}} \EE{q_{d,n,w}(\z)}{\p{y_{d,n}|\z,\thetab}}.
\end{aligned}
\end{equation}
We attributed to each subject the diagnostic class with the highest inferred probability.

The performance on test datasets was evaluated by measuring the classification accuracy (\%).
All results were validated by means of $5$-folds cross-validation.

\paragraph{Results}
In \tabref{tab:classifier} we show the classification accuracy of MCVAE and our model on the experimental conditions described earlier.
There are $7$ cases in the MTL condition that could be fitted with our model, which is not possible to do with the MCVAE.
In $5$ of them we measure on average a better performance with respect to the ST + TL condition.
In all the cases where the classification accuracy is $> 50\%$ our model performs either similarly or statistically better then the MCVAE.

