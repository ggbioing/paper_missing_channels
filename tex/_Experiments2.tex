%%%%%%%%%%%%%%%%%%%%%
%% MEDICAL IMAGING %%
%%%%%%%%%%%%%%%%%%%%%
\section{Experiments on Medical Imaging Datasets}
\label{sec:real}

\input{./tex/tab/table_datasets_numbers.tex}
% \input{./tex/tab/table_datasets_cross_validation.tex}
% \input{./tex/tab/table_datasets_prediction.tex}
\input{./tex/tab/table_datasets_prediction_combat.tex}
\input{./tex/tab/table_model_comparison.tex}
\input{./tex/tab/table_datasets_prediction_dx.tex}
% \input{./tex/tab/table_model_comparison.tex}

Joint modeling multiple datasets in real scenarios represents an hybrid case between modeling missing at random (MAR) and missing not at random (MNAR) observations, cases which we separately treated in \S\ref{sec:synth}.
Indeed, every dataset may lack of views which are instead present in others by default.
At the same time, in each dataset, there may be missing views for some data-points due to unknown random reasons.

In this section we describe our results on jointly modeling real medical imaging datasets, independently acquired in the context of research studies on human cognitive decline.

We executed three kinds of experiments:
1) multi-view feature prediction with our model on all the available datasets;
2) multi-view feature prediction with our model and benchmark methods on the dataset with the highest number of data-points and views;
3) diagnosis classification accuracy with our model on all the available datasets.

\subsection{Data Sources}
Data used in the preparation of this section were obtained from the following sources.
\begin{enumerate}
%
\item From the Alzheimer's Disease Neuroimaging Initiative (ADNI)
\footnote{
\href{http://adni.loni.usc.edu}{adni.loni.usc.edu}.
The ADNI was launched in 2003 as a public-private partnership, led by Principal Investigator Michael W. Weiner, MD. For up-to-date information, see \href{www.adni-info.org}{www.adni-info.org}.
},
a database of brain imaging and related clinical data of cognitively normal subjects and at various stages of cognitive decline.
%
\item From MIRIAD dataset \cite{Miriad}, a database of brain imaging and related clinical data of cognitively normal subjects and data of Alzheimer's.
%
\item From OASIS-3 dataset \citep{oasis3}, a database of brain MRI and PET scans and related clinical data of cognitively normal subjects and subjects at various stages of cognitive decline.
%
\item From a local cohort collected at the University Hospitals, with brain MRI and PET scans and related clinical data of healthy subjects and subjects with various cognitive disorders.
\end{enumerate}
We divided the ADNI dataset into two independent ones:
`adni1', composed by subjects recruited in the initial study,
and `adni2' composed by those subjects subsequently recruited in ADNI-GO, ADNI2, and ADNI3.
Since data modalities and acquisition protocols of `adni1' are different from those of `adni2', we consider these two cohorts as independent datasets.

\subsection{Imaging Processing}
All the brain scans were processed in order to have measurements on regions defined in the Desikan-Killiany atlas \citep{Desikan2006}.
All brain MRI scans, except for the ones in the local cohort, were processed with FreeSurfer \footnote{\href{https://surfer.nmr.mgh.harvard.edu/}{surfer.nmr.mgh.harvard.edu}} \citep{freesurfer} to measure brain cortical and sub-cortical volumes.
MRIs in the local cohort were processed with SPM \citep{Ashburner2000} to measure gray matter density in the brain, averaged in the regions of interest defined in the chosen atlas.
Relative standardized uptake values (SUVR) were computed for all the PET scans, processed with SPM.
SUVRs were computed using the cerebellum as reference region, and averaged in each region of the atlas.

\subsection{Gathering Observations into Views}
From the five datasets (`adni1', `adni2', `miriad', `oasis3', `local') we grouped the observations into the following views.
1) `clin': grouping age and the Mini-Mental cognitive score (MMSE).
2) `mri': grouping brain volumes computed with FreeSurfer.
3) `fdg': average brain glucose uptake measured through the analysis of FDG-PET scans.
4) `av45': average brain amyloid uptake measured through the analysis of AV45-PET scans.
5) `tau': average brain protein Tau uptake measured through the analysis of TAU-PET scans.
%6) `mri local': this is a view only present in the `local' dataset, composed by measurements on brain gray matter density derived from SPM \cite{Ashburner2000}.
6) `mri local': this is a view only present in the `local' dataset, composed by measurements of brain gray matter density.

All datasets except `miriad' are composed by multiple time points, or follow ups, for each subject.
To met the model assumption of independent datapoints, we kept only one time point per subject.
Specifically, we choose the first time point available, or baseline, for `adni1' and `adni2' subjects.
In `oasis3', since measurements were mostly acquired in different days, we choose to join nearby time points into a single one.
Time interval between views within one subject was minimal (`av45' vs `mri': $\leq 90$ days, `mri' vs `clin': $\leq 90$ days).

In \tabref{table:datasets} we show the number of observations stratified by dataset and view.
Size of the intersection ($\cap$) and union ($\cup$) of subjects with available views is also provided.
Please note that the only view in common across datasets is the clinical one, composed by MMSE and age features only.

We normalized all the views feature-wise with the ComBat method \citep{Fortin2018}, an harmonization technique useful for reduce unwanted sources of variation in the data due to the difference in acquisition protocols among datasets.
In ComBat, we set the `age' as main regressor, and `adni2' as reference dataset for the training set.
As we used cross-validation to validate our results, the ComBat reference dataset for the testing was the training split.

\subsection{Experiment 1: Feature Prediction}
For the model architecture, we choose the likelihood functions as in \eqnref{eq:decoder}.
We then choose to parametrize the encoder as:

We fit our multi-view model one dataset at a time, and lastly on the whole dataset pool.
We will refer to this last model as `all'.
Observations were divided into five splits, stratified to keep an intra-dataset proportion of views as close as possible as the ones of the original datasets.
After setting a dropout threshold to $0.5$ we measured, through the mean squared error, the prediction error of the dataset-specific models on the testing hold-out observations of the same dataset (within).
To have a measure of the generalization ability for each model, we measured the prediction accuracy also on all the test sets not belonging to the same dataset of the trainig set (cross).
The same was done with the model `all'.
In this case the hold out observations belong to the whole dataset pool and we will refer to its testing set as the `joint' set.

\subsection{Experiment 2: Benchmark Comparison}
\ldots

\subsection{Experiment 3: Diagnosis Prediction}
\ldots

\subsection{Results}
In \tabref{tab:features} we show for each trained multi-view model, the prediction error on the within-dataset test sets, cross-dataset test sets, and joint set, whenever computable.
We notice that the generalization performance of the model `all' is almost always the best in predicting the `clin' and `mri' views.
These are also the views most represented in all the datasets.
We also note that joint model systematically outperforms the dataset-specific models when applyed across dataset.
Moreover it generally provides better predictions even respect to the within-dataset results.
On the other extreme, views belonging only to a specific dataset, such as `fdg' in `adni2' and `mri local' in the `local' dataset are better predicted with dataset-specific models.
In the remainig cases of `av45' and `tau', the joint model seems, on average, to perform better than the single dataset-specific ones.


