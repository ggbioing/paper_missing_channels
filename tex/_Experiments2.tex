%%%%%%%%%%%%%%%%%%%%%
%% MEDICAL IMAGING %%
%%%%%%%%%%%%%%%%%%%%%
\section{Experiments on Medical Imaging Datasets}
\label{sec:real}

% \input{./tex/tab/table_datasets_cross_validation.tex}
% \input{./tex/tab/table_datasets_prediction.tex}
% \input{./tex/tab/table_model_comparison.tex}

Joint modeling multiple datasets in real scenarios represents an hybrid case between modeling missing at random (MAR) and missing not at random (MNAR) observations, cases which we separately treated in \S\ref{sec:synth}.
Indeed, every dataset may lack of views which are instead present in others by default.
At the same time, in each dataset, there may be missing views for some data-points due to unknown random reasons.

In this section we describe our results on jointly modeling real medical imaging datasets, independently acquired in the context of research studies on human cognitive decline.

We executed two kinds of experiments:
1) benchmark evaluation of our model against the best competing methods from the previous section;
2) multi-view feature prediction with our model on all the available datasets in multiple conditions.
%  3) diagnosis classification accuracy with our model on all the available datasets.

\subsection{Data Sources}
\label{ssec:datasets}

Data used in the preparation of this section were obtained from the following sources.
\begin{enumerate}
%
\item From the Alzheimer's Disease Neuroimaging Initiative (ADNI)
\footnote{
\href{http://adni.loni.usc.edu}{adni.loni.usc.edu}.
The ADNI was launched in 2003 as a public-private partnership, led by Principal Investigator Michael W. Weiner, MD. For up-to-date information, see \href{www.adni-info.org}{www.adni-info.org}.
},
a database of brain imaging and related clinical data of cognitively normal subjects and at various stages of cognitive decline.
%
\item From MIRIAD dataset \cite{Miriad}, a database of brain imaging and related clinical data of cognitively normal subjects and data of Alzheimer's.
%
\item From OASIS-3 dataset \citep{oasis3}, a database of brain MRI and PET scans and related clinical data of cognitively normal subjects and subjects at various stages of cognitive decline.
%
\item From a local cohort collected at the University Hospitals, with brain MRI and PET scans and related clinical data of healthy subjects and subjects with various cognitive disorders.
\end{enumerate}
We divided the ADNI dataset into two independent ones:
`adni1', composed by subjects recruited in the initial study,
and `adni2' composed by those subjects subsequently recruited in ADNI-GO, ADNI2, and ADNI3.
Since data modalities and acquisition protocols of `adni1' are different from those of `adni2', we consider these two cohorts as independent datasets.

To summarize, we grouped our data into five distinct datasets which we named as follows: `adni1', `adni2', `miriad', `oasis3', `local'.

\subsection{Imaging Processing}
All the brain scans were processed in order to have measurements on regions defined in the Desikan-Killiany atlas \citep{Desikan2006}.
All brain MRI scans, except for the ones in the local cohort, were processed with FreeSurfer \footnote{\href{https://surfer.nmr.mgh.harvard.edu/}{surfer.nmr.mgh.harvard.edu}} \citep{freesurfer} to measure brain cortical and sub-cortical volumes.
MRIs in the local cohort were processed with SPM \citep{Ashburner2000} to measure gray matter density in the brain, averaged in the regions of interest defined in the chosen atlas.
Relative standardized uptake values (SUVR) were computed for all the PET scans, processed with SPM.
SUVRs were computed using the cerebellum as reference region, and averaged in each region of the atlas.

\subsection{Gathering Observations into Views}
\label{ssec:views}
\input{./tex/tab/table_datasets_numbers.tex}

Observations from the five available datasets (\S~\ref{ssec:datasets}) were grouped into the following views.
1) `clin': grouping age and the Mini-Mental cognitive score (MMSE).
2) `mri': grouping brain volumes computed with FreeSurfer.
3) `fdg': average brain glucose uptake measured through the analysis of FDG-PET scans.
4) `av45': average brain amyloid uptake measured through the analysis of AV45-PET scans.
5) `tau': average brain protein Tau uptake measured through the analysis of TAU-PET scans.
%6) `mri local': this is a view only present in the `local' dataset, composed by measurements on brain gray matter density derived from SPM \cite{Ashburner2000}.
6) `mri local': this is a view only present in the `local' dataset, composed by measurements of brain gray matter density.

All datasets except `miriad' are composed by multiple time points, or follow ups, for each subject.
To met the model assumption of independent datapoints, we kept only one time point per subject.
Specifically, we choose the first time point available, or baseline, for subjects belonging to the `adni1', `adni2', and `local' datasets.
In `oasis3', since measurements were mostly acquired in different days, we choose to join nearby time points into a single one.
Time interval between views within one subject was minimal (`av45' vs `mri': $\leq 90$ days, `mri' vs `clin': $\leq 90$ days).

In \tabref{table:datasets} we show the number of observations stratified by dataset and view.
Size of the intersection ($\cap$) and union ($\cup$) of subjects with available views is also provided.
Please note that the only view in common across datasets is the clinical one, composed by MMSE and age features only.

We normalized all the views feature-wise with the ComBat method \citep{Fortin2018}, an harmonization technique useful for reduce unwanted sources of variation in the data due to the difference in acquisition protocols among datasets.
In ComBat, we set the `age' as main regressor, and `adni2' as reference dataset for the training set.
As we used cross-validation to validate our results, the ComBat reference dataset for the testing was the training split.

A final feature-wise standardization step was applied by zero centering the data and by rescaling them to have a unity variance.
Standardization parameters were computed on the training sets and applied to the training and test sets.


\subsection{Experiment 1: Benchmark Validation}
\input{./tex/tab/table_model_comparison.tex}

The purpose of this experiment is to validate on real data the benchmarked results obtained with the synthetic experiments (\S~\ref{sec:synth}).

As benchmark methods to compare to, we choose the best performers on the synthetic experiments, namely knn5 and DAE.
We choose for our model a linear Gaussian parameterization for the likelihood and variational distributions, respectively as in \eqnref{eq:decoder} and \eqnref{eq:dropout_posterior}.

We trained all the models (knn5, dae, ours) with data coming from all the datasets except from `adni2', leaved out for testing purposes.
We choose the `adni2' dataset as testing dataset, as the results would be supported by the dataset with most of the views, and with generally the highest number of observations per view (\tabref{table:datasets}).

Prediction performances were evaluated with the Mean Squared Error (MSE) metric, measured on the available views in the testing dataset, reconstructed with \eqnref{eq:reconstruction}.
All results were validated by means of $5$-folds cross-validation.

\paragraph{Results}
In \tabref{tab:model_comparison}, we show the MSE metric on predicting missing views in the testing dataset with our model and with the benckmark ones.
Best results in boldface, which show a clear advantage of using our model, confirm our findings in the synthetic experiments.

\subsection{Experiment 2: Feature Prediction}
\label{ssec:feats}
\input{./tex/tab/table_datasets_prediction_combat.tex}

The purpose of this experiment is to investigate the generalization performance of our model when tested on unseen datasets.

For each testing dataset, chosen among the five available ones (\S~\ref{ssec:datasets}), and for each view, this experiment was run in three different conditions depending on the provenance of the training data:
1) from the same dataset as the testing dataset, whose results represents an internal benchmark for our model;
2) from a different dataset, chosen among the remaining available ones;
3) from all the remaining available datasets, merged together into a single one.
By removing the cases where views are not simultaneously present in both the training and testing set,
we are left with $16$ possible scenarios where all the three experimental conditions can be simultaneously tested an thus compared.

We choose for our model a linear Gaussian parameterization for the likelihood and variational distributions, respectively as in \eqnref{eq:decoder} and \eqnref{eq:dropout_posterior}.
Models were trained on all the available views in the training dataset, independently of their presence in the testing dataset.
Prediction performances were evaluated with the Mean Squared Error (MSE) metric, measured on the available views in the testing dataset, reconstructed with \eqnref{eq:reconstruction}.
All results were validated by means of $5$-folds cross-validation.

\paragraph{Results}
In \tabref{tab:features} we show the prediction error in terms of MSE for each test dataset and view, on the three experimental conditions described earlier.
Me measure on average that the third condition, where we train our model on data merged from multiple datasets, gives results which are in line or even better than our internal benchmark condition, where instead training data comes from the same dataset used in testing.

%  \subsection{Experiment 3: Diagnosis Prediction}
%  \input{./tex/tab/table_datasets_prediction_dx.tex}
%  
%  The purpose of this experiment is to investigate the generalization performance of our model in predicting on subjects of unseen datasets their diagnostic class among:
%  Alzheimer's disease (AD),
%  mild cognitive impairment (MCI),
%  normal cognition (NC).
%  
%  The experimental setup about the choice of training and testing data is the same described in the previous experiment (\S~\ref{ssec:feats}).
%  
%  We choose a linear Gaussian parameterization for the variational distributions as in \eqnref{eq:dropout_posterior}.
%  To adapt our model to this new classification experiment, we adopt as decoding function for the latent variable $\z$, the following Categorical likelihood:
%  \begin{align}
%  \label{eq:classifier}
%  \p{y_{d,n}|\z,\thetab} = \operatorname{Cat}\left(\mathbf{\pi}=\thetab\z\right),
%  \end{align}
%  where $y_{d,n}$ is the diagnosis associated to the data-point $n$ in the dataset $d$, and where the probability vector $\mathbf{\pi}$ is parametrized with a linear transformation of the latent $\z$ by the matrix $\thetab$.
%  
%  Models were trained on all the available views in the training dataset, independently of their presence in the testing dataset.
%  Classes probabilities were inferred from the all the available views in the testing dataset with the following equation:
%  \begin{equation}\label{eq:reconstructiony}
%  \begin{aligned}
%  \hat{y}_{d,n} = \frac{1}{V_{d, n}} \sum_{w=1}^{V_{d, n}} \EE{q_{d,n,w}(\z)}{\p{y_{d,n}|\z,\thetab}}.
%  \end{aligned}
%  \end{equation}
%  We attributed to each subject the diagnostic class with the highest inferred probability.
%  
%  The performance of our model on test datasets was evaluated by measuring the classification accuracy (\%).
%  All results were validated by means of $5$-folds cross-validation.
%  
