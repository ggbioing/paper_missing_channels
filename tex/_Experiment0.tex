\subsection{Illustration on a simplified brain imaging dataset}
\label{sec:proof_of_concept}

In this section we describe a simple experiment where we use MT-MCVAE to model the joint relationship between
magnetic resonance imaging (MRI) and fluorodeoxyglucose positron emission tomography (FDG-PET) images when there are missing data at training time.
The trained model will be then applied on test data
to the cross-modality reconstruction problem (MRI to FDG-PET and \textit{vice versa}).

Data comes from the `Adni2' dataset (see details in \S~\ref{ssec:datasets}, \tabref{table:datasets}), from which we took the MRI and FDG-PET brain imaging modalities.
In what follows, each one of this two modalities corresponds to a specific data view.
For each subject ($n=424$, with both MRI and FDG-PET) we extracted $3$ brain slices for each one of the sagittal, coronal, and axial plane.
The resulting $3816$ slices were randomly allocated to a training and testing set with respectively sizes of $3500$ and $316$ samples.
We downsampled the slices to dimension $28 \times 28$ ($784$ pixels).
%
To simulate a datasets with missing views, we controlled for the fraction of observations with complete views ($f$) in the training set:
this procedure is depicted in \figref{fig:nimg_scheme} where we show an example of training dataset created with $f=1/3$.
%
For our experiments we took all the $3500$ training images and we randomly removed MRI and FDG-PET views to obtain different training sets for which $f \in \{0, 0.25, 0.5, 0.75, 1\}$.
In the case $f = 0$, from each subjects we kept either its MRI or FDG-PET slices, representing the limit case where no direct relationship between views is observable.
In the limit case $f = 1$, all MRI and FDG-PET are paired, representing the ideal case of no missing views, that is the working case of the MCVAE \citep{Antelmi2019}.
We adopted a deep architecture with $4$ layers for both encoders and decoders, having ReLU activation functions and layer dimensions of $784-1024-1024-16$ in the encoding and $16-1024-1024-784$ in the decoding path,
an architecture inspired from those used by \cite{dcca1} and \cite{dcca2} for a similar task on the MNIST dataset \citep{mnist}.
We adopted a Gaussian likelihood for the decoders, with independent diagonal covariance parameters, and we trained our model with mini-batches of size $500$ for $3000$ epochs, after setting up the Adam optimizer with a learning rate of 0.001.
Training was repeated $5$ times, by changing the initialization random seed of the model parameters.

\input{./tex/fig/nimg.tex}
\input{./tex/tab/table_nimg.tex}
In \tabref{tab:nimg} we show the Mean Squared Error (MSE) and Negative Log-Likelihood (NLL) when predicting MRI from the FDG-PET slices and \textit{vice versa} in the testing set.
We notice the immediate drop in the error metrics as soon as the parameter $f$ increases,
which means that as the model is fed with an increasing proportion of multi-view data points in the training set,
its predictions on the testing set become more precise.
%
% In \figref{fig:nimg_test} is possible to visually inspect these predictions.

